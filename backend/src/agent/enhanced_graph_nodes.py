"""
Enhanced Graph nodes - integrated intelligent Firecrawl content enhancement functionality
"""

import os
import json
from typing import List, Dict, Any
from datetime import datetime
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import AIMessage

from agent.state import OverallState, ReflectionState
from agent.content_enhancement_decision import (
    get_content_enhancement_decision_maker,
    EnhancementDecision
)
from agent.utils import get_research_topic


def content_enhancement_analysis(state: OverallState, config: RunnableConfig) -> dict:
    """
    Intelligent content enhancement analysis node - decide whether to use Firecrawl for deep scraping
    
    This node will:
    1. Analyze the quality of current research results
    2. Evaluate whether deep content enhancement is needed
    3. Select priority URLs for Firecrawl scraping
    4. Execute content enhancement (if needed)
    5. Merge enhanced content into research results
    """
    
    try:
        # Get current research context
        plan = state.get("plan", [])
        current_pointer = state.get("current_task_pointer", 0)
        
        # Determine research topic
        if plan and current_pointer < len(plan):
            research_topic = plan[current_pointer]["description"]
        else:
            research_topic = state.get("user_query") or get_research_topic(state["messages"])
        
        # Get current research findings
        current_findings = state.get("web_research_result", [])
        
        # Get grounding sources (extract from recent search results)
        grounding_sources = []
        sources_gathered = state.get("sources_gathered", [])
        for source in sources_gathered[-10:]:  # Latest 10 sources
            if isinstance(source, dict):
                grounding_sources.append({
                    "title": source.get("title", ""),
                    "url": source.get("url", ""),
                    "snippet": source.get("snippet", "")
                })
        
        print(f"ğŸ¤” Analyzing content enhancement requirements...")
        print(f"  Research topic: {research_topic}")
        print(f"  Current findings count: {len(current_findings)}")
        print(f"  Available information sources: {len(grounding_sources)}")
        
        # Use intelligent decision maker for analysis
        decision = get_content_enhancement_decision_maker().analyze_enhancement_need(
            research_topic=research_topic,
            current_findings=current_findings,
            grounding_sources=grounding_sources,
            config=config
        )
        
        print(f"ğŸ“Š Enhancement decision results:")
        print(f"  Needs enhancement: {decision.needs_enhancement}")
        print(f"  Confidence: {decision.confidence_score:.2f}")
        print(f"  Enhancement type: {decision.enhancement_type}")
        print(f"  Priority URL count: {len(decision.priority_urls)}")
        
        # Save decision to state
        state_update = {
            "enhancement_decision": {
                "needs_enhancement": decision.needs_enhancement,
                "confidence_score": decision.confidence_score,
                "enhancement_type": decision.enhancement_type,
                "reasoning": decision.reasoning,
                "priority_urls": decision.priority_urls
            }
        }
        
        # If enhancement is not needed, return directly
        if not decision.needs_enhancement:
            print("âœ… Current content quality is sufficient, no enhancement needed")
            state_update["enhancement_status"] = "skipped"
            return state_update
        
        # If no Firecrawl API Key, skip enhancement
        if not get_content_enhancement_decision_maker().firecrawl_app:
            print("âš ï¸ Missing FIRECRAWL_API_KEY, skipping content enhancement")
            state_update["enhancement_status"] = "skipped_no_api"
            return state_update
        
        # Execute content enhancement
        print(f"ğŸ”¥ Executing Firecrawl content enhancement...")
        enhanced_results = []
        
        # Synchronous call (temporarily simplified, can be changed to async later)
        for url_info in decision.priority_urls:
            url = url_info.get("url")
            if not url:
                continue
            
            try:
                print(f"  Scraping: {url_info.get('title', 'Unknown')}")
                
                result = get_content_enhancement_decision_maker().firecrawl_app.scrape_url(url)
                
                if result and result.success:
                    markdown_content = result.markdown or ''
                    
                    enhanced_results.append({
                        "url": url,
                        "title": url_info.get("title", ""),
                        "original_priority": url_info.get("priority_score", 0),
                        "enhanced_content": markdown_content,
                        "content_length": len(markdown_content),
                        "source_type": "firecrawl_enhanced",
                        "timestamp": datetime.now().isoformat()
                    })
                    
                    print(f"    âœ… Success: {len(markdown_content)} characters")
                else:
                    print(f"    âŒ Failed: {result.error if hasattr(result, 'error') else 'Unknown error'}")
                    
            except Exception as e:
                print(f"    âŒ Exception: {str(e)}")
                continue
        
        if enhanced_results:
            # Add enhanced content to research results
            enhanced_contents = []
            for result in enhanced_results:
                # Format enhanced content
                formatted_content = f"""

## Deep Content Enhancement - {result['title']}

Source: {result['url']}
Content length: {result['content_length']} characters

{result['enhanced_content'][:3000]}{'...' if len(result['enhanced_content']) > 3000 else ''}

---
"""
                enhanced_contents.append(formatted_content)
            
            state_update.update({
                "enhanced_content_results": enhanced_results,
                "web_research_result": enhanced_contents,  # æ·»åŠ åˆ°ç ”ç©¶ç»“æœä¸­
                "enhancement_status": "completed",
                "enhanced_sources_count": len(enhanced_results)
            })
            
            print(f"âœ… å†…å®¹å¢å¼ºå®Œæˆ: {len(enhanced_results)} ä¸ªæº")
        else:
            print("âŒ å†…å®¹å¢å¼ºå¤±è´¥ï¼Œæ²¡æœ‰æˆåŠŸæŠ“å–ä»»ä½•å†…å®¹")
            state_update["enhancement_status"] = "failed"
        
        return state_update
        
    except Exception as e:
        error_message = f"å†…å®¹å¢å¼ºåˆ†æèŠ‚ç‚¹å¼‚å¸¸: {str(e)}"
        print(f"âŒ {error_message}")
        return {
            "enhancement_status": "error",
            "enhancement_error": error_message
        }


def should_enhance_content(state: OverallState) -> str:
    """
    æ¡ä»¶è¾¹å‡½æ•° - å†³å®šæ˜¯å¦è¿›å…¥å†…å®¹å¢å¼ºæµç¨‹
    
    åŸºäºä»¥ä¸‹æ¡ä»¶åˆ¤æ–­:
    1. æ˜¯å¦é…ç½®äº†Firecrawl API Key
    2. å½“å‰ç ”ç©¶å¾ªç¯æ¬¡æ•°
    3. ç”¨æˆ·é…ç½®çš„å¢å¼ºåå¥½
    """
    
    # æ£€æŸ¥Firecrawlå¯ç”¨æ€§
    if not os.getenv("FIRECRAWL_API_KEY"):
        print("âš ï¸ è·³è¿‡å†…å®¹å¢å¼º: æœªé…ç½®FIRECRAWL_API_KEY")
        return "continue_without_enhancement"
    
    # æ£€æŸ¥ç ”ç©¶å¾ªç¯æ¬¡æ•°ï¼ˆé¿å…åœ¨æ—©æœŸå¾ªç¯ä¸­å¢å¼ºï¼‰
    research_loop_count = state.get("research_loop_count", 0)
    if research_loop_count < 1:  # è‡³å°‘è¿›è¡Œä¸€è½®ç ”ç©¶åå†è€ƒè™‘å¢å¼º
        print(f"âš ï¸ è·³è¿‡å†…å®¹å¢å¼º: ç ”ç©¶å¾ªç¯æ¬¡æ•°ä¸è¶³ ({research_loop_count})")
        return "continue_without_enhancement"
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»è¿›è¡Œè¿‡å¢å¼ºï¼ˆé¿å…é‡å¤å¢å¼ºï¼‰
    if state.get("enhancement_status") in ["completed", "skipped"]:
        print("âš ï¸ è·³è¿‡å†…å®¹å¢å¼º: å·²ç»å®Œæˆå¢å¼º")
        return "continue_without_enhancement"
    
    # æ£€æŸ¥å½“å‰å‘ç°æ•°é‡ï¼ˆè‡³å°‘è¦æœ‰ä¸€äº›åŸºç¡€å†…å®¹ï¼‰
    current_findings = state.get("web_research_result", [])
    if len(current_findings) < 1:
        print("âš ï¸ è·³è¿‡å†…å®¹å¢å¼º: ç¼ºå°‘åŸºç¡€ç ”ç©¶å†…å®¹")
        return "continue_without_enhancement"
    
    print("âœ… æ»¡è¶³å¢å¼ºæ¡ä»¶ï¼Œè¿›å…¥å†…å®¹å¢å¼ºåˆ†æ")
    return "analyze_enhancement_need"


def enhanced_reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """
    å¢å¼ºç‰ˆåæ€èŠ‚ç‚¹ - åœ¨åŸæœ‰reflectionåŸºç¡€ä¸Šè€ƒè™‘å†…å®¹å¢å¼ºçš„ç»“æœ
    """
    
    # å…ˆè°ƒç”¨åŸæœ‰çš„reflectioné€»è¾‘
    from agent.graph import reflection
    reflection_result = reflection(state, config)
    
    # å¦‚æœè¿›è¡Œäº†å†…å®¹å¢å¼ºï¼Œè°ƒæ•´reflectionçš„åˆ¤æ–­
    enhancement_status = state.get("enhancement_status")
    enhanced_sources_count = state.get("enhanced_sources_count", 0)
    
    if enhancement_status == "completed" and enhanced_sources_count > 0:
        print(f"ğŸ“ˆ å†…å®¹å¢å¼ºå®Œæˆï¼Œè°ƒæ•´åæ€åˆ¤æ–­")
        print(f"  å¢å¼ºäº† {enhanced_sources_count} ä¸ªä¿¡æ¯æº")
        
        # å¦‚æœæˆåŠŸå¢å¼ºäº†å†…å®¹ï¼Œæ›´å€¾å‘äºè®¤ä¸ºä¿¡æ¯å……è¶³
        # ä½†ä»ç„¶ä¿ç•™LLMçš„åˆ¤æ–­æƒé‡
        if not reflection_result["is_sufficient"]:
            # ç»™å¢å¼ºå†…å®¹ä¸€å®šçš„"åŠ åˆ†"
            enhancement_boost = min(enhanced_sources_count * 0.3, 0.8)
            print(f"  ç”±äºå†…å®¹å¢å¼ºï¼Œæå‡å……è¶³æ€§è¯„ä¼° (+{enhancement_boost:.1f})")
            
            # å¦‚æœå¢å¼ºæ•ˆæœå¾ˆå¥½ï¼Œå¯èƒ½å°†"ä¸å……è¶³"æ”¹ä¸º"å……è¶³"
            if enhancement_boost >= 0.6:
                print("  âœ… åŸºäºå†…å®¹å¢å¼ºç»“æœï¼Œåˆ¤å®šä¿¡æ¯å·²å……è¶³")
                reflection_result["is_sufficient"] = True
                reflection_result["knowledge_gap"] = "å†…å®¹å·²é€šè¿‡æ·±åº¦æŠ“å–å¾—åˆ°å……åˆ†è¡¥å……"
    
    elif enhancement_status == "skipped":
        print("ğŸ“ å†…å®¹å¢å¼ºè¢«è·³è¿‡ï¼Œä½¿ç”¨åŸå§‹åæ€ç»“æœ")
    
    elif enhancement_status == "failed":
        print("âš ï¸ å†…å®¹å¢å¼ºå¤±è´¥ï¼Œå¯èƒ½éœ€è¦æ›´å¤šç ”ç©¶å¾ªç¯")
    
    return reflection_result


# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–å¢å¼ºå†³ç­–ä¿¡æ¯ç”¨äºæ—¥å¿—
def format_enhancement_decision_log(decision: EnhancementDecision) -> str:
    """æ ¼å¼åŒ–å¢å¼ºå†³ç­–ä¿¡æ¯ç”¨äºæ—¥å¿—è¾“å‡º"""
    
    log_lines = [
        f"ğŸ“Š å†…å®¹å¢å¼ºå†³ç­–æŠ¥å‘Š:",
        f"  å†³ç­–: {'éœ€è¦å¢å¼º' if decision.needs_enhancement else 'æ— éœ€å¢å¼º'}",
        f"  ç½®ä¿¡åº¦: {decision.confidence_score:.2f}",
        f"  å¢å¼ºç±»å‹: {decision.enhancement_type}",
        f"  ä¼˜å…ˆURLæ•°é‡: {len(decision.priority_urls)}"
    ]
    
    if decision.priority_urls:
        log_lines.append("  ä¼˜å…ˆURLs:")
        for i, url_info in enumerate(decision.priority_urls, 1):
            log_lines.append(f"    {i}. {url_info.get('title', 'N/A')} (è¯„åˆ†: {url_info.get('priority_score', 0):.2f})")
    
    log_lines.append(f"  æ¨ç†: {decision.reasoning[:200]}...")
    
    return "\n".join(log_lines) 